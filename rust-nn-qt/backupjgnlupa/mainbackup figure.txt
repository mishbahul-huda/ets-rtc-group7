use ndarray::{Array2, Axis};
use ndarray_rand::RandomExt;
use rand_distr::StandardNormal;
use rand::thread_rng;
use csv::ReaderBuilder;
use std::error::Error;
use plotters::prelude::*;
use std::sync::{Arc, Mutex};
use std::thread;
use eframe;

mod frontend;
use frontend::NeuralNetworkApp;

pub const EPOCHS: usize = 1000;
const LR: f64 = 0.01;
const HIDDEN: usize = 16;
const LOG_INTERVAL: usize = 100;

fn relu(x: &Array2<f64>) -> Array2<f64> {
    x.mapv(|v| v.max(0.0))
}

fn relu_deriv(x: &Array2<f64>) -> Array2<f64> {
    x.mapv(|v| if v > 0.0 { 1.0 } else { 0.0 })
}

fn sigmoid(x: &Array2<f64>) -> Array2<f64> {
    x.mapv(|v| 1.0 / (1.0 + (-v).exp()))
}

fn binary_cross_entropy(y_pred: &Array2<f64>, y_true: &Array2<f64>) -> f64 {
    let eps = 1e-7;
    let y_pred_clipped = y_pred.mapv(|v| v.max(eps).min(1.0 - eps));
    let loss = y_true * &y_pred_clipped.mapv(|v| v.ln()) +
               (1.0 - y_true) * &y_pred_clipped.mapv(|v| (1.0 - v).ln());
    -loss.mean().unwrap()
}

fn load_data(path: &str) -> Result<(Array2<f64>, Array2<f64>), Box<dyn Error>> {
    let mut rdr = ReaderBuilder::new().has_headers(true).from_path(path)?;

    let mut features: Vec<Vec<f64>> = Vec::new();
    let mut labels: Vec<f64> = Vec::new();

    for result in rdr.records() {
        let record = result?;
        let vals: Result<Vec<f64>, _> = record.iter().map(|s| s.trim().parse::<f64>()).collect();
        if let Ok(vals) = vals {
            let (x, y) = vals.split_at(vals.len() - 1);
            features.push(x.to_vec());
            labels.push(y[0]);
        }
    }

    let feature_array = Array2::from_shape_vec((features.len(), features[0].len()), features.concat())?;
    let label_array = Array2::from_shape_vec((labels.len(), 1), labels)?;

    Ok((feature_array, label_array))
}

fn plot_loss(losses: &[f64]) -> Result<(), Box<dyn Error>> {
    // Create result directory if it doesn't exist
    std::fs::create_dir_all("result")?;

    let root = BitMapBackend::new("result/lossfigure.png", (640, 480)).into_drawing_area();
    root.fill(&WHITE)?;

    let max_loss = losses.iter().cloned().fold(f64::NAN, f64::max);
    let mut chart = ChartBuilder::on(&root)
        .caption("Training Loss", ("sans-serif", 30))
        .margin(20)
        .x_label_area_size(30)
        .y_label_area_size(40)
        .build_cartesian_2d(0..EPOCHS, 0.0..max_loss)?;

    chart.configure_mesh().draw()?;

    chart.draw_series(LineSeries::new(
        losses.iter().enumerate().map(|(i, &loss)| (i, loss)),
        &RED,
    ))?;

    Ok(())
}

fn train_neural_network(
    app: Arc<Mutex<NeuralNetworkApp>>,
) -> Result<(), Box<dyn Error>> {
    let (x, y_true) = load_data("csv/pollution_datasettt.csv")?;
    let (n_samples, n_features) = x.dim();

    let mut rng = thread_rng();
    let mut w1 = Array2::random_using((n_features, HIDDEN), StandardNormal, &mut rng);
    let mut b1 = Array2::zeros((1, HIDDEN));
    let mut w2 = Array2::random_using((HIDDEN, 1), StandardNormal, &mut rng);
    let mut b2 = Array2::zeros((1, 1));

    let mut losses = Vec::new();

    let mut final_pred = Array2::zeros((n_samples, 1));

    for epoch in 0..EPOCHS {
        let z1 = x.dot(&w1) + &b1;
        let a1 = relu(&z1);
        let z2 = a1.dot(&w2) + &b2;
        let y_pred = sigmoid(&z2);

        let loss = binary_cross_entropy(&y_pred, &y_true);
        losses.push(loss);

        let dz2 = &y_pred - &y_true;
        let dw2 = a1.t().dot(&dz2) / n_samples as f64;
        let db2 = dz2.sum_axis(Axis(0)) / n_samples as f64;

        let da1 = dz2.dot(&w2.t());
        let dz1 = da1 * relu_deriv(&z1);
        let dw1 = x.t().dot(&dz1) / n_samples as f64;
        let db1 = dz1.sum_axis(Axis(0)) / n_samples as f64;

        w1 -= &(dw1 * LR);
        b1 -= &(db1 * LR);
        w2 -= &(dw2 * LR);
        b2 -= &(db2 * LR);

        final_pred = y_pred.clone();

        // Calculate accuracy periodically
        if epoch % LOG_INTERVAL == 0 || epoch == EPOCHS - 1 {
            let predictions = y_pred.mapv(|v| if v >= 0.5 { 1.0 } else { 0.0 });
            let correct = predictions
                .iter()
                .zip(y_true.iter())
                .filter(|(p, y)| (*p - *y).abs() < 1e-6)
                .count();
            let accuracy = (correct as f64 / n_samples as f64) * 100.0;
            
            // Update progress with accuracy
            app.lock().unwrap().update_progress(epoch, loss, accuracy);
        } else {
            // Update progress without accuracy
            app.lock().unwrap().update_progress(epoch, loss, -1.0);
        }
        
        // Small sleep to give UI time to breathe
        std::thread::sleep(std::time::Duration::from_millis(1));
    }

    // Save loss plot to file
    plot_loss(&losses)?;

    // Calculate final accuracy
    let predictions = final_pred.mapv(|v| if v >= 0.5 { 1.0 } else { 0.0 });
    let correct = predictions
        .iter()
        .zip(y_true.iter())
        .filter(|(p, y)| (*p - *y).abs() < 1e-6)
        .count();

    let accuracy = (correct as f64 / n_samples as f64) * 100.0;
    
    // Mark training as completed
    app.lock().unwrap().training_completed(accuracy);

    Ok(())
}

fn main() -> Result<(), Box<dyn Error>> {
    // Create application options with a default window size
    let native_options = eframe::NativeOptions {
        viewport: eframe::egui::ViewportBuilder::default()
            .with_inner_size([800.0, 600.0]),
        ..Default::default()
    };
    
    // Create app and set up training
    let app = NeuralNetworkApp::new();
    
    // We need to move the app setup into the eframe creation callback
    // to ensure proper lifetimes
    eframe::run_native(
        "Neural Network Training",
        native_options,
        Box::new(|_cc| {
            // Clone the app and wrap in Arc<Mutex<>>
            let app_wrapped = Arc::new(Mutex::new(app));
            let app_clone = app_wrapped.clone();
            
            // Set up training callback
            {
                let mut app_locked = app_wrapped.lock().unwrap();
                app_locked.handle_train_click(move || {
                    let app_training = app_clone.clone();
                    
                    // Run training in a separate thread
                    thread::spawn(move || {
                        if let Err(e) = train_neural_network(app_training) {
                            eprintln!("Training error: {}", e);
                        }
                    });
                });
            }
            
            // Create a new instance before dropping the lock
            let app_ui = {
                let app_locked = app_wrapped.lock().unwrap();
                app_locked.clone()
            };
            
            Box::new(app_ui)
        }),
    )?;
    
    Ok(())
}